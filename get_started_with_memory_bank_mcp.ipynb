{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": "# Get started with Vertex AI Memory Bank MCP\n\n<table align=\"left\">\n  <td style=\"text-align: center\">\n    <a href=\"https://colab.research.google.com/github/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\">\n      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n    </a>\n  </td>\n  <td style=\"text-align: center\">\n    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Finardini%2Fvertex-memory-bank-mcp%2Fmain%2Fget_started_with_memory_bank_mcp.ipynb\">\n      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n    </a>\n  </td>\n  <td style=\"text-align: center\">\n    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/inardini/vertex-memory-bank-mcp/main/get_started_with_memory_bank_mcp.ipynb\">\n      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n    </a>\n  </td>\n  <td style=\"text-align: center\">\n    <a href=\"https://github.com/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\">\n      <img width=\"32px\" src=\"https://storage.googleapis.com/github-repo/generative-ai/logos/GitHub_Invertocat_Dark.svg\" alt=\"GitHub logo\"><br> View on GitHub\n    </a>\n  </td>\n</table>\n\n<div style=\"clear: both;\"></div>\n\n<p>\n<b>Share to:</b>\n\n<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\" target=\"_blank\">\n  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n</a>\n\n<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\" target=\"_blank\">\n  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n</a>\n\n<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\" target=\"_blank\">\n  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n</a>\n\n<a href=\"https://reddit.com/submit?url=https%3A//github.com/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\" target=\"_blank\">\n  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n</a>\n\n<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/inardini/vertex-memory-bank-mcp/blob/main/get_started_with_memory_bank_mcp.ipynb\" target=\"_blank\">\n  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n</a>\n</p>"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Ivan Nardini](https://github.com/inardini) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial is about showing how to use Vertex AI Memory Bank MCP server and how to integrate it with Gemini.\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "- **Use Vertex AI Memory Bank with Python MCP Client** - Direct integration using the MCP protocol\n",
    "- **Use Vertex AI Memory Bank with Gemini** - Building memory-augmented AI applications\n",
    "\n",
    "This guide provides **code** that:\n",
    "- Connects to the actual Vertex AI Memory Bank MCP server\n",
    "- Executes memory operations\n",
    "- Demonstrates how to use Vertex AI Memory Bank MCP server to give memory to Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xgsnfzx137eL"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Warning:</b> This example is for demostration purpose only. Vertex AI Memory Bank MCP server is not a Google product. And it is not officially support.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK and other required packages\n",
    "\n",
    "Let's install all the necessary packages for the different integration methods we'll explore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFy3H3aPgx12"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet \"google-genai>=1.40.0\" \"mcp[cli]>=1.0.0\" \"google-cloud-aiplatform>=1.118.0\" \"pydantic>=2.0.0\" \"python-dotenv>=1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"inardini-demos\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "client = vertexai.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this notebook locally (not in Colab), you also need to set up your project information and your Google Cloud credentials in the `.env`.  \n",
    "\n",
    "Create a `.env` file in the same directory as this notebook with the following content:\n",
    "\n",
    "```\n",
    "# Google Cloud Configuration\n",
    "GOOGLE_CLOUD_PROJECT=your-project-id\n",
    "GOOGLE_CLOUD_LOCATION=us-central1\n",
    "\n",
    "# Authentication for Gemini (choose one)\n",
    "# Option 1: Vertex AI with service Account Key\n",
    "GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json\n",
    "\n",
    "# Option 2: Google AI Studio with API Key \n",
    "# GOOGLE_API_KEY=your-api-key\n",
    "\n",
    "# Optional: Existing Agent Engine name\n",
    "# AGENT_ENGINE_NAME=projects/PROJECT_ID/locations/LOCATION/reasoningEngines/ENGINE_ID\n",
    "```\n",
    "\n",
    "Then run the following code to verify the configuration. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file if it exists\n",
    "if os.path.exists(\".env\"):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    # Verify environment variables are set\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_PROJECT\"):\n",
    "        print(\"Warning: GOOGLE_CLOUD_PROJECT not set. Please configure your .env file.\")\n",
    "    if not os.environ.get(\"GOOGLE_CLOUD_LOCATION\"):\n",
    "        print(\"Warning: GOOGLE_CLOUD_LOCATION not set. Using default: us-central1\")\n",
    "    if not os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\") and not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "        print(\"Warning: GOOGLE_APPLICATION_CREDENTIALS or GOOGLE_API_KEY not set. Please configure your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables \n",
    "If you're running this notebook locally (not in Colab), you'll need \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the necessary Python libraries for our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# MCP imports\n",
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "from google import genai\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHi9MqGB2_8l"
   },
   "source": [
    "## Set Vertex AI Memory Bank MCP server and its path\n",
    "\n",
    "Set the MCP server path. Our client will start this script as a subprocess and communicate with it via stdio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_DJG3Ah2_8l"
   },
   "outputs": [],
   "source": [
    "# Path to your Memory Bank MCP server\n",
    "# Adjust this path to where you have the memory_bank_server.py file\n",
    "MEMORY_BANK_SERVER_PATH = os.path.abspath(\"memory_bank_server.py\")\n",
    "\n",
    "if not os.path.exists(MEMORY_BANK_SERVER_PATH):\n",
    "    print(f\"Memory Bank server not found at: {MEMORY_BANK_SERVER_PATH}\")\n",
    "    print(\"Please ensure memory_bank_server.py is in the current directory\")\n",
    "else:\n",
    "    print(f\"Memory Bank server found at: {MEMORY_BANK_SERVER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAFBd4T-4MVg"
   },
   "source": [
    "## Understanding MCP Client Connection\n",
    "\n",
    "To connect with MCP servers you need a client using:\n",
    "\n",
    "- **stdio** - Communication through standard input/output (local servers)\n",
    "- **SSE** - Server-Sent Events for remote servers\n",
    "\n",
    "This Memory Bank server uses stdio, making it perfect for local development and containerized deployments.\n",
    "\n",
    "For our client, you configure StdioServerParameters with init method, telling the MCP client how to start the server using memory_bank_server.py script.\n",
    "\n",
    "After that, you use connect to start the server process, established the stdio communication channels (read, write), initialize the MCP session, and then lists the tools the server has advertised.\n",
    "\n",
    "Finally, you have call_tool method to send a request to execute a tool (like create_memory) with specific arguments and then parses the JSON response from the server's output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgtKpEgU4eZd"
   },
   "outputs": [],
   "source": [
    "class MemoryBankMCPClient:\n",
    "    \"\"\"A client for interacting with the Memory Bank MCP server.\"\"\"\n",
    "\n",
    "    def __init__(self, server_path: str, project_id: str, location: str):\n",
    "        \"\"\"Initialize the Memory Bank MCP client.\"\"\"\n",
    "        self.project_id = project_id\n",
    "        self.location = location\n",
    "        self.server_params = StdioServerParameters(\n",
    "            command=sys.executable,\n",
    "            args=[server_path],\n",
    "            env={\n",
    "                \"GOOGLE_CLOUD_PROJECT\": self.project_id,\n",
    "                \"GOOGLE_CLOUD_LOCATION\": self.location\n",
    "            }\n",
    "        )\n",
    "        self.session = None\n",
    "        self.initialized = False\n",
    "        self.stdio_cm = None\n",
    "        self.session_cm = None\n",
    "\n",
    "    async def connect(self):\n",
    "        \"\"\"Establish connection to the MCP server.\"\"\"\n",
    "        # Store the context managers\n",
    "        self.stdio_cm = stdio_client(self.server_params)\n",
    "        self.read, self.write = await self.stdio_cm.__aenter__()\n",
    "\n",
    "        self.session_cm = ClientSession(self.read, self.write)\n",
    "        self.session = await self.session_cm.__aenter__()\n",
    "\n",
    "        # Initialize the MCP session\n",
    "        await self.session.initialize()\n",
    "        print(\"Connected to Memory Bank MCP server\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    async def disconnect(self):\n",
    "        \"\"\"Close the connection to the MCP server.\"\"\"\n",
    "        if self.session_cm:\n",
    "            await self.session_cm.__aexit__(None, None, None)\n",
    "        if self.stdio_cm:\n",
    "            await self.stdio_cm.__aexit__(None, None, None)\n",
    "        print(\"\\nDisconnected from Memory Bank MCP server\")\n",
    "\n",
    "    async def call_tool(self, tool_name: str, arguments: dict) -> Any:\n",
    "        \"\"\"Call a tool on the MCP server.\"\"\"\n",
    "        if not self.session:\n",
    "            raise RuntimeError(\"Not connected to MCP server\")\n",
    "\n",
    "        result = await self.session.call_tool(tool_name, arguments)\n",
    "\n",
    "        # Extract the actual content from CallToolResult\n",
    "        if hasattr(result, 'content') and result.content:\n",
    "            # Get the text content from the first content item\n",
    "            if result.content[0].type == 'text':\n",
    "                import json\n",
    "                return json.loads(result.content[0].text)\n",
    "\n",
    "        return result\n",
    "\n",
    "    async def initialize_memory_bank(self, memory_topics: List[str] = None):\n",
    "        \"\"\"Initialize the Memory Bank.\"\"\"\n",
    "        if not self.initialized:\n",
    "            result = await self.call_tool(\n",
    "                \"initialize_memory_bank\",\n",
    "                {\n",
    "                    \"project_id\": os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
    "                    \"location\": os.environ[\"GOOGLE_CLOUD_LOCATION\"],\n",
    "                    \"memory_topics\": memory_topics or [\"USER_PREFERENCES\", \"USER_PERSONAL_INFO\"]\n",
    "                }\n",
    "            )\n",
    "            self.initialized = True\n",
    "            return result\n",
    "        return {\"status\": \"already_initialized\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNPO1bm22_8m"
   },
   "source": [
    "## Use Vertex AI Memory Bank MCP for basic operations\n",
    "\n",
    "Now that we define our MCP client, we can test our Vertex AI Memory Bank with a \"Hello, World!\" example. The example demonstrates the full lifecycle of interacting with the service:\n",
    "\n",
    "- Create an instance of our client and connects to the Vertex AI Memory Bank MCP server.\n",
    "- Create a new Vertex AI Memory Bank instance on Agent Engine.\n",
    "- Run through some core CRUD (Create, Read, Update, Delete) operations, showing how to generate (from a conversation) and retrieve (with a smart search).\n",
    "- Clean up the server subprocess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7PdEP0YD970"
   },
   "outputs": [],
   "source": [
    "async def demo_basic_operations():\n",
    "    \"\"\"Demonstrate basic Memory Bank operations via MCP.\"\"\"\n",
    "\n",
    "    # Create and connect client\n",
    "    client = MemoryBankMCPClient(\n",
    "        server_path=MEMORY_BANK_SERVER_PATH,\n",
    "        project_id=PROJECT_ID,\n",
    "        location=LOCATION\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        await client.connect()\n",
    "\n",
    "        # Initialize Memory Bank\n",
    "        print(\"\\n Initializing Memory Bank...\")\n",
    "        init_result = await client.initialize_memory_bank()\n",
    "        print(f\"Result: {json.dumps(init_result, indent=2)[:200]}...\")\n",
    "\n",
    "        # Generate memories from a conversation\n",
    "        print(\"\\n Generating memories from conversation...\")\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": \"Hi, I'm Alice. I work as a data scientist in San Francisco.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Nice to meet you, Alice! How can I help you today?\"},\n",
    "            {\"role\": \"user\", \"content\": \"I'm interested in learning about machine learning with Python.\"}\n",
    "        ]\n",
    "\n",
    "        gen_result = await client.call_tool(\n",
    "            \"generate_memories\",\n",
    "            {\n",
    "                \"conversation\": conversation,\n",
    "                \"scope\": {\"user_id\": \"alice_demo_123\"},\n",
    "                \"wait_for_completion\": True\n",
    "            }\n",
    "        )\n",
    "        print(f\"Generated memories: {json.dumps(gen_result, indent=2)[:300]}...\")\n",
    "\n",
    "        # Retrieve memories\n",
    "        print(\"\\n Retrieving memories...\")\n",
    "        retrieve_result = await client.call_tool(\n",
    "            \"retrieve_memories\",\n",
    "            {\n",
    "                \"scope\": {\"user_id\": \"alice_demo_123\"},\n",
    "                \"search_query\": \"programming interests\",\n",
    "                \"top_k\": 3\n",
    "            }\n",
    "        )\n",
    "        print(f\"Retrieved: {json.dumps(retrieve_result, indent=2)[:300]}...\")\n",
    "\n",
    "    finally:\n",
    "        await client.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUOHfbhxGRiV"
   },
   "outputs": [],
   "source": [
    "await demo_basic_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0Qfw7_bGbQR"
   },
   "source": [
    "## Using Memory Bank with Gemini and MCP\n",
    "\n",
    "Now that you are a bit familiar with the server, let's integrate Memory Bank with Google's Gemini API using MCP, enabling automatic function calling and memory-augmented AI responses.\n",
    "\n",
    "We start with establishing an MCP session as before. Then we create a session and we give Gemini a complex, multi-step prompt: \"Create two memories for me, then retrieve them all to verify.\"\n",
    "\n",
    "Instead of us manually calling create_memory twice and then retrieve_memories, the Gemini model intelligently parses the prompt, identifies the required tools from the MCP session, and executes them in the correct sequence to fulfill the request.\n",
    "\n",
    "The Gemini SDK handles the entire back-and-forth tool calling loop for us. This is a massive simplification that lets you build complex, multi-tool agents with natural language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgqEFVBjbJ2e"
   },
   "outputs": [],
   "source": [
    "async def demo_automatic_tool_calling():\n",
    "    \"\"\"Demonstrate Gemini's automatic tool calling with Memory Bank.\"\"\"\n",
    "\n",
    "    # Create Gemini client\n",
    "    client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # Create MCP server parameters\n",
    "    server_params = StdioServerParameters(\n",
    "        command=sys.executable,\n",
    "        args=[MEMORY_BANK_SERVER_PATH],\n",
    "        env={\n",
    "            \"GOOGLE_CLOUD_PROJECT\": PROJECT_ID,\n",
    "            \"GOOGLE_CLOUD_LOCATION\": LOCATION\n",
    "        }\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize MCP session\n",
    "            await session.initialize()\n",
    "\n",
    "            print(\"Initializing Memory Bank...\\n\")\n",
    "\n",
    "            # Initialize Memory Bank first\n",
    "            init_result = await session.call_tool(\n",
    "                \"initialize_memory_bank\",\n",
    "                {\n",
    "                    \"project_id\": PROJECT_ID,\n",
    "                    \"location\": LOCATION,\n",
    "                    \"memory_topics\": [\"USER_PREFERENCES\", \"USER_PERSONAL_INFO\"]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if hasattr(init_result, 'content') and init_result.content:\n",
    "                init_data = json.loads(init_result.content[0].text)\n",
    "                engine_name = init_data.get('agent_engine_name')\n",
    "                print(f\"Memory Bank initialized with engine:\\n   {engine_name}\\n\")\n",
    "\n",
    "            # Prompt that will trigger automatic tool calling\n",
    "            prompt = f\"\"\"\n",
    "            Please help me manage my memories. Here's what I need you to do:\n",
    "\n",
    "            1. Create a memory that I (user_id: \"gemini_demo_user\") prefer dark mode in all applications\n",
    "            2. Create another memory that I (user_id: \"gemini_demo_user\") love Python programming\n",
    "            3. Then retrieve all memories for user_id: \"gemini_demo_user\" to verify they were saved\n",
    "\n",
    "            Show me a summary of what you found.\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"Sending request to Gemini with automatic tool calling enabled...\\n\")\n",
    "\n",
    "            # Gemini will automatically call MCP tools to fulfill the request\n",
    "            response = await client.aio.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],  # Pass MCP session directly - SDK handles everything!\n",
    "                    # Automatic function calling is enabled by default\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\"=\" * 100)\n",
    "            print(\"Gemini Response:\")\n",
    "            print(\"=\" * 100)\n",
    "            print(response.text)\n",
    "            print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demo_automatic_tool_calling():\n",
    "    \"\"\"Demonstrate Gemini's automatic tool calling with Memory Bank.\"\"\"\n",
    "\n",
    "    # Create Gemini client\n",
    "    client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "    # Create MCP server parameters\n",
    "    server_params = StdioServerParameters(\n",
    "        command=sys.executable,\n",
    "        args=[MEMORY_BANK_SERVER_PATH],\n",
    "        env={\n",
    "            \"GOOGLE_CLOUD_PROJECT\": PROJECT_ID,\n",
    "            \"GOOGLE_CLOUD_LOCATION\": LOCATION\n",
    "        }\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize MCP session\n",
    "            await session.initialize()\n",
    "\n",
    "            print(\"Initializing Memory Bank...\\n\")\n",
    "\n",
    "            # Initialize Memory Bank first\n",
    "            init_result = await session.call_tool(\n",
    "                \"initialize_memory_bank\",\n",
    "                {\n",
    "                    \"project_id\": PROJECT_ID,\n",
    "                    \"location\": LOCATION,\n",
    "                    \"memory_topics\": [\"USER_PREFERENCES\", \"USER_PERSONAL_INFO\"]\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if hasattr(init_result, 'content') and init_result.content:\n",
    "                init_data = json.loads(init_result.content[0].text)\n",
    "                engine_name = init_data.get('agent_engine_name')\n",
    "                print(f\"Memory Bank initialized with engine:\\n   {engine_name}\\n\")\n",
    "\n",
    "            # Turn 1: Store memories\n",
    "            print(\"=\" * 100)\n",
    "            print(\"TURN 1: Storing memories\")\n",
    "            print(\"=\" * 100)\n",
    "            \n",
    "            store_prompt = \"\"\"\n",
    "            Please create two memories for me:\n",
    "            1. I (user_id: \"gemini_demo_user\") prefer dark mode in all applications\n",
    "            2. I (user_id: \"gemini_demo_user\") love Python programming\n",
    "            \n",
    "            Confirm when the memories are stored.\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"Sending request to Gemini to store memories...\\n\")\n",
    "\n",
    "            response1 = await client.aio.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=store_prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\"Gemini Response:\")\n",
    "            print(response1.text)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            # Turn 2: Retrieve memories\n",
    "            print(\"=\" * 100)\n",
    "            print(\"TURN 2: Retrieving memories\")\n",
    "            print(\"=\" * 100)\n",
    "            \n",
    "            retrieve_prompt = \"\"\"\n",
    "            Now retrieve all memories for user_id: \"gemini_demo_user\" and show me what you found.\n",
    "            \"\"\"\n",
    "\n",
    "            print(\"Sending request to Gemini to retrieve memories...\\n\")\n",
    "\n",
    "            response2 = await client.aio.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",\n",
    "                contents=retrieve_prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=[session],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\"Gemini Response:\")\n",
    "            print(response2.text)\n",
    "            print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHhtrnOAuS7z"
   },
   "source": [
    "Let's see Gemini with Vertex AI Memory Bank MCP in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YrazbTMYub72"
   },
   "outputs": [],
   "source": [
    "await demo_automatic_tool_calling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To avoid incurring charges to your Google Cloud account, delete the Agent Engine resources created in this tutorial.\n",
    "\n",
    "The following code deletes all Agent Engines created during this notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all agent engines with rate limiting to avoid quota exhaustion\n",
    "import time\n",
    "\n",
    "# List all agent engines\n",
    "agent_engines = list(client.agent_engines.list())\n",
    "print(f\"Found {len(agent_engines)} Agent Engine(s) to delete.\\n\")\n",
    "\n",
    "# Delete each agent engine with delay between requests\n",
    "deleted_count = 0\n",
    "for i, engine in enumerate(agent_engines, 1):\n",
    "    try:\n",
    "        client.agent_engines.delete(name=engine.api_resource.name, force=True)\n",
    "        print(f\"[{i}/{len(agent_engines)}] Deleted Agent Engine: {engine.api_resource.name}\")\n",
    "        deleted_count += 1\n",
    "\n",
    "        # Wait 10 seconds between deletions to avoid default quota limits\n",
    "        if i < len(agent_engines):\n",
    "            print(f\"Waiting 10 seconds before next deletion to respect quota limits...\")\n",
    "            time.sleep(10)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}/{len(agent_engines)}] Failed to delete {engine.api_resource.name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mb-mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}